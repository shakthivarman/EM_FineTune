# Default configuration for embedding finetuning

model:
  name: "sentence-transformers/all-MiniLM-L6-v2"
  max_length: 512
  pooling_mode: "mean"  # Options: mean, cls, max
  normalize_embeddings: true

training:
  loss_function: "MultipleNegativesRankingLoss"
  epochs: 3
  batch_size: 32
  learning_rate: 2e-5
  weight_decay: 0.01
  warmup_steps: 500
  max_grad_norm: 1.0
  
optimization:
  use_lora: true
  mixed_precision: "fp16"  # Options: no, fp16, bf16
  gradient_checkpointing: true
  
lora:
  r: 16
  lora_alpha: 32
  target_modules: ["query", "key", "value", "dense"]
  lora_dropout: 0.1
  bias: "none"

data:
  format: "pairs"  # Options: pairs, triplets, single
  negative_sampling: true
  num_negatives: 1
  val_split: 0.1

logging:
  eval_steps: 500
  save_steps: 1000
  logging_steps: 100
  use_wandb: false
  wandb_project: "embedding-finetuning"
  log_level: "INFO"

output:
  dir: "./output"

# Loss function specific parameters
loss_kwargs:
  MultipleNegativesRankingLoss:
    scale: 20.0
  ContrastiveLoss:
    margin: 1.0
    distance_metric: "euclidean"
  TripletLoss:
    margin: 0.5
    distance_metric: "euclidean"
  InfoNCELoss:
    temperature: 0.07
  AnglELoss:
    w1: 1.0
    w2: 1.0
    w3: 1.0
    angle_tau: 1.0
    cosine_tau: 20.0

# Hardware settings
hardware:
  seed: 42
  num_workers: 4  # For data loading